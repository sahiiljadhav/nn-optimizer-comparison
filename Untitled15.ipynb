{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPas42Mo1BlmMkhsLDIyttH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahiiljadhav/nn-optimizer-comparison/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "1PcIFk4WBjKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "2wJOo759COwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Loss function\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
        "\n",
        "def categorical_crossentropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jEfrzcIgCh31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Synthetic Dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_classes=3, n_informative=4, n_redundant=0, random_state=42)\n",
        "one_hot = OneHotEncoder()\n",
        "y_encoded = one_hot.fit_transform(y.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "pqPbA_M0C1I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XDb0_DloDK2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network architecture\n",
        "np.random.seed(42)\n",
        "input_neurons = 4\n",
        "hidden_neurons1 = 3\n",
        "hidden_neurons2 = 4\n",
        "output_neurons = 3"
      ],
      "metadata": {
        "id": "m_0JXyDQDSRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "weights = {\n",
        "    \"W1\": np.random.randn(input_neurons, hidden_neurons1) * 0.01,\n",
        "    \"W2\": np.random.randn(hidden_neurons1, hidden_neurons2) * 0.01,\n",
        "    \"W3\": np.random.randn(hidden_neurons2, output_neurons) * 0.01\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    \"b1\": np.zeros((1, hidden_neurons1)),\n",
        "    \"b2\": np.zeros((1, hidden_neurons2)),\n",
        "    \"b3\": np.zeros((1, output_neurons))\n",
        "}\n"
      ],
      "metadata": {
        "id": "r-EqzhfbDZeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_propagation(X, weights, biases):\n",
        "    Z1 = np.dot(X, weights[\"W1\"]) + biases[\"b1\"]\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, weights[\"W2\"]) + biases[\"b2\"]\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(A2, weights[\"W3\"]) + biases[\"b3\"]\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3"
      ],
      "metadata": {
        "id": "FXSn129gDbMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward propagation\n",
        "def backward_propagation(X, y, Z1, A1, Z2, A2, Z3, A3, weights):\n",
        "    m = X.shape[0]\n",
        "    dZ3 = categorical_crossentropy_derivative(y, A3)\n",
        "    dW3 = np.dot(A2.T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA2 = np.dot(dZ3, weights[\"W3\"].T)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA1 = np.dot(dZ2, weights[\"W2\"].T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}"
      ],
      "metadata": {
        "id": "04-AJzSmDeom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer functions\n",
        "def gradient_descent(weights, biases, gradients, lr):\n",
        "    for key in weights.keys():\n",
        "        weights[key] -= lr * gradients[\"d\" + key]\n",
        "        biases[key.replace(\"W\", \"b\")] -= lr * gradients[\"d\" + key.replace(\"W\", \"b\")]\n",
        "\n",
        "def train_nn(optimizer, epochs=100, lr=0.01):\n",
        "    global weights, biases\n",
        "    history = []\n",
        "    for epoch in range(epochs):\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X_train, weights, biases)\n",
        "        gradients = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, weights)\n",
        "\n",
        "        optimizer(weights, biases, gradients, lr)\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_loss = categorical_crossentropy(y_train, A3)\n",
        "        train_accuracy = np.mean(np.argmax(A3, axis=1) == np.argmax(y_train, axis=1))\n",
        "        history.append((train_loss, train_accuracy))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "_xpbOv8wDhq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "train_nn(gradient_descent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-lSIGd1DnFu",
        "outputId": "525f8e28-0ec9-4807-81b5-70b783d94895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0986124233240318, 0.26375),\n",
              " (1.0986099111000127, 0.34625),\n",
              " (1.0986074158088202, 0.34625),\n",
              " (1.098604939197796, 0.34625),\n",
              " (1.0986024792890037, 0.34625),\n",
              " (1.0986000361847084, 0.34625),\n",
              " (1.0985976092594552, 0.34625),\n",
              " (1.0985951983267148, 0.34625),\n",
              " (1.0985928033018288, 0.34625),\n",
              " (1.0985904240697542, 0.34625),\n",
              " (1.0985880606793332, 0.34625),\n",
              " (1.0985857129155454, 0.34625),\n",
              " (1.09858338073437, 0.34625),\n",
              " (1.0985810643583054, 0.34625),\n",
              " (1.098578763681313, 0.34625),\n",
              " (1.0985764783544536, 0.34625),\n",
              " (1.0985742083068937, 0.34625),\n",
              " (1.098571953464515, 0.34625),\n",
              " (1.0985697136701598, 0.34625),\n",
              " (1.0985674890435717, 0.34625),\n",
              " (1.098565279401888, 0.34625),\n",
              " (1.0985630846233376, 0.34625),\n",
              " (1.0985609046250266, 0.34625),\n",
              " (1.0985587392158918, 0.34625),\n",
              " (1.0985565882227506, 0.34625),\n",
              " (1.0985544515167052, 0.34625),\n",
              " (1.0985523291860124, 0.34625),\n",
              " (1.0985502210712241, 0.34625),\n",
              " (1.0985481270574597, 0.34625),\n",
              " (1.0985460472231485, 0.34625),\n",
              " (1.0985439812809945, 0.34625),\n",
              " (1.0985419289734943, 0.34625),\n",
              " (1.0985398903914032, 0.34625),\n",
              " (1.0985378655890383, 0.34625),\n",
              " (1.0985358543892128, 0.34625),\n",
              " (1.0985338566531422, 0.34625),\n",
              " (1.098531872274965, 0.34625),\n",
              " (1.0985299010960166, 0.34625),\n",
              " (1.0985279430267547, 0.34625),\n",
              " (1.0985259980849276, 0.34625),\n",
              " (1.0985240661543023, 0.34625),\n",
              " (1.0985221470830475, 0.34625),\n",
              " (1.0985202407617114, 0.34625),\n",
              " (1.0985183471754822, 0.34625),\n",
              " (1.0985164662183045, 0.34625),\n",
              " (1.0985145977926374, 0.34625),\n",
              " (1.098512741798472, 0.34625),\n",
              " (1.0985108982396448, 0.34625),\n",
              " (1.098509067098134, 0.34625),\n",
              " (1.0985072482810578, 0.34625),\n",
              " (1.0985054416429976, 0.34625),\n",
              " (1.098503647139578, 0.34625),\n",
              " (1.0985018646118692, 0.34625),\n",
              " (1.0985000939707872, 0.34625),\n",
              " (1.0984983351266344, 0.34625),\n",
              " (1.098496588087433, 0.34625),\n",
              " (1.0984948527088116, 0.34625),\n",
              " (1.0984931289144126, 0.34625),\n",
              " (1.0984914166278572, 0.34625),\n",
              " (1.0984897157621087, 0.34625),\n",
              " (1.0984880262240526, 0.34625),\n",
              " (1.098486347966064, 0.34625),\n",
              " (1.098484680914046, 0.34625),\n",
              " (1.098483025013389, 0.34625),\n",
              " (1.0984813802322209, 0.34625),\n",
              " (1.098479746464318, 0.34625),\n",
              " (1.0984781236067434, 0.34625),\n",
              " (1.0984765116043536, 0.34625),\n",
              " (1.0984749103863103, 0.34625),\n",
              " (1.0984733198352787, 0.34625),\n",
              " (1.098471739946677, 0.34625),\n",
              " (1.0984701706666111, 0.34625),\n",
              " (1.098468611870834, 0.34625),\n",
              " (1.098467063488883, 0.34625),\n",
              " (1.098465525451899, 0.34625),\n",
              " (1.0984639976948478, 0.34625),\n",
              " (1.0984624801644989, 0.34625),\n",
              " (1.0984609727826389, 0.34625),\n",
              " (1.0984594754804915, 0.34625),\n",
              " (1.098457988214807, 0.34625),\n",
              " (1.0984565109093523, 0.34625),\n",
              " (1.0984550434778024, 0.34625),\n",
              " (1.0984535858544247, 0.34625),\n",
              " (1.0984521379742558, 0.34625),\n",
              " (1.0984506997785102, 0.34625),\n",
              " (1.0984492712241165, 0.34625),\n",
              " (1.0984478522402954, 0.34625),\n",
              " (1.0984464427435499, 0.34625),\n",
              " (1.0984450426706989, 0.34625),\n",
              " (1.0984436519592309, 0.34625),\n",
              " (1.09844227055445, 0.34625),\n",
              " (1.098440898413337, 0.34625),\n",
              " (1.0984395354624794, 0.34625),\n",
              " (1.0984381816332116, 0.34625),\n",
              " (1.0984368368751982, 0.34625),\n",
              " (1.0984355011119913, 0.34625),\n",
              " (1.0984341742940995, 0.34625),\n",
              " (1.0984328563616272, 0.34625),\n",
              " (1.0984315472455979, 0.34625),\n",
              " (1.09843024688762, 0.34625)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}