{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdxf5PFnBC+mrjzA4H8rxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahiiljadhav/nn-optimizer-comparison/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a neural network from scratch for a multiclass classification\n",
        "task with the following architecture:<br>\n",
        "\n",
        "\n",
        "Input layer: 4 neurons<br>\n",
        "First hidden layer: 3 neurons<br>\n",
        "Second hidden layer: 4 neurons<br>\n",
        "Output layer: 3 neurons<br>\n",
        "\n",
        "\n",
        "\n",
        "Generate a random dataset for a 3-class classification problem. Apply\n",
        "the designed neural network on generated dataset using the ReLU activation\n",
        "function in the hidden layers and the softmax activation function in the output\n",
        "layer for multiclass classification. Use categorical cross-entropy as the loss\n",
        "function for training. Additionally, implement the Gradient Descent, Momentum\n",
        "based GD, NAG, AdaGrad, RMS prop, Adam optimizer as part of the training\n",
        "process. Train the neural network using all optimizers and evaluate their\n",
        "performance on the dataset using performance metrics accuracy."
      ],
      "metadata": {
        "id": "Mdz-7hGwBT-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "1PcIFk4WBjKm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "2wJOo759COwX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "# Loss function\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
        "\n",
        "def categorical_crossentropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jEfrzcIgCh31"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Synthetic Dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_classes=3, n_informative=4, n_redundant=0, random_state=42)\n",
        "one_hot = OneHotEncoder()\n",
        "y_encoded = one_hot.fit_transform(y.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "pqPbA_M0C1I-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XDb0_DloDK2e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network architecture\n",
        "np.random.seed(42)\n",
        "input_neurons = 4\n",
        "hidden_neurons1 = 3\n",
        "hidden_neurons2 = 4\n",
        "output_neurons = 3"
      ],
      "metadata": {
        "id": "m_0JXyDQDSRO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "weights = {\n",
        "    \"W1\": np.random.randn(input_neurons, hidden_neurons1) * 0.01,\n",
        "    \"W2\": np.random.randn(hidden_neurons1, hidden_neurons2) * 0.01,\n",
        "    \"W3\": np.random.randn(hidden_neurons2, output_neurons) * 0.01\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    \"b1\": np.zeros((1, hidden_neurons1)),\n",
        "    \"b2\": np.zeros((1, hidden_neurons2)),\n",
        "    \"b3\": np.zeros((1, output_neurons))\n",
        "}\n"
      ],
      "metadata": {
        "id": "r-EqzhfbDZeG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_propagation(X, weights, biases):\n",
        "    Z1 = np.dot(X, weights[\"W1\"]) + biases[\"b1\"]\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, weights[\"W2\"]) + biases[\"b2\"]\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(A2, weights[\"W3\"]) + biases[\"b3\"]\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3"
      ],
      "metadata": {
        "id": "FXSn129gDbMV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward propagation\n",
        "def backward_propagation(X, y, Z1, A1, Z2, A2, Z3, A3, weights):\n",
        "    m = X.shape[0]\n",
        "    dZ3 = categorical_crossentropy_derivative(y, A3)\n",
        "    dW3 = np.dot(A2.T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA2 = np.dot(dZ3, weights[\"W3\"].T)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA1 = np.dot(dZ2, weights[\"W2\"].T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}"
      ],
      "metadata": {
        "id": "04-AJzSmDeom"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer functions\n",
        "def gradient_descent(weights, biases, gradients, lr):\n",
        "    for key in weights.keys():\n",
        "        weights[key] -= lr * gradients[\"d\" + key]\n",
        "        biases[key.replace(\"W\", \"b\")] -= lr * gradients[\"d\" + key.replace(\"W\", \"b\")]\n",
        "\n",
        "def train_nn(optimizer, epochs=100, lr=0.01):\n",
        "    global weights, biases\n",
        "    history = []\n",
        "    for epoch in range(epochs):\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X_train, weights, biases)\n",
        "        gradients = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, Z3, A3, weights)\n",
        "\n",
        "        optimizer(weights, biases, gradients, lr)\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        train_loss = categorical_crossentropy(y_train, A3)\n",
        "        train_accuracy = np.mean(np.argmax(A3, axis=1) == np.argmax(y_train, axis=1))\n",
        "        history.append((train_loss, train_accuracy))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "_xpbOv8wDhq1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "train_nn(gradient_descent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-lSIGd1DnFu",
        "outputId": "525f8e28-0ec9-4807-81b5-70b783d94895"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0986124233240318, 0.26375),\n",
              " (1.0986099111000127, 0.34625),\n",
              " (1.0986074158088202, 0.34625),\n",
              " (1.098604939197796, 0.34625),\n",
              " (1.0986024792890037, 0.34625),\n",
              " (1.0986000361847084, 0.34625),\n",
              " (1.0985976092594552, 0.34625),\n",
              " (1.0985951983267148, 0.34625),\n",
              " (1.0985928033018288, 0.34625),\n",
              " (1.0985904240697542, 0.34625),\n",
              " (1.0985880606793332, 0.34625),\n",
              " (1.0985857129155454, 0.34625),\n",
              " (1.09858338073437, 0.34625),\n",
              " (1.0985810643583054, 0.34625),\n",
              " (1.098578763681313, 0.34625),\n",
              " (1.0985764783544536, 0.34625),\n",
              " (1.0985742083068937, 0.34625),\n",
              " (1.098571953464515, 0.34625),\n",
              " (1.0985697136701598, 0.34625),\n",
              " (1.0985674890435717, 0.34625),\n",
              " (1.098565279401888, 0.34625),\n",
              " (1.0985630846233376, 0.34625),\n",
              " (1.0985609046250266, 0.34625),\n",
              " (1.0985587392158918, 0.34625),\n",
              " (1.0985565882227506, 0.34625),\n",
              " (1.0985544515167052, 0.34625),\n",
              " (1.0985523291860124, 0.34625),\n",
              " (1.0985502210712241, 0.34625),\n",
              " (1.0985481270574597, 0.34625),\n",
              " (1.0985460472231485, 0.34625),\n",
              " (1.0985439812809945, 0.34625),\n",
              " (1.0985419289734943, 0.34625),\n",
              " (1.0985398903914032, 0.34625),\n",
              " (1.0985378655890383, 0.34625),\n",
              " (1.0985358543892128, 0.34625),\n",
              " (1.0985338566531422, 0.34625),\n",
              " (1.098531872274965, 0.34625),\n",
              " (1.0985299010960166, 0.34625),\n",
              " (1.0985279430267547, 0.34625),\n",
              " (1.0985259980849276, 0.34625),\n",
              " (1.0985240661543023, 0.34625),\n",
              " (1.0985221470830475, 0.34625),\n",
              " (1.0985202407617114, 0.34625),\n",
              " (1.0985183471754822, 0.34625),\n",
              " (1.0985164662183045, 0.34625),\n",
              " (1.0985145977926374, 0.34625),\n",
              " (1.098512741798472, 0.34625),\n",
              " (1.0985108982396448, 0.34625),\n",
              " (1.098509067098134, 0.34625),\n",
              " (1.0985072482810578, 0.34625),\n",
              " (1.0985054416429976, 0.34625),\n",
              " (1.098503647139578, 0.34625),\n",
              " (1.0985018646118692, 0.34625),\n",
              " (1.0985000939707872, 0.34625),\n",
              " (1.0984983351266344, 0.34625),\n",
              " (1.098496588087433, 0.34625),\n",
              " (1.0984948527088116, 0.34625),\n",
              " (1.0984931289144126, 0.34625),\n",
              " (1.0984914166278572, 0.34625),\n",
              " (1.0984897157621087, 0.34625),\n",
              " (1.0984880262240526, 0.34625),\n",
              " (1.098486347966064, 0.34625),\n",
              " (1.098484680914046, 0.34625),\n",
              " (1.098483025013389, 0.34625),\n",
              " (1.0984813802322209, 0.34625),\n",
              " (1.098479746464318, 0.34625),\n",
              " (1.0984781236067434, 0.34625),\n",
              " (1.0984765116043536, 0.34625),\n",
              " (1.0984749103863103, 0.34625),\n",
              " (1.0984733198352787, 0.34625),\n",
              " (1.098471739946677, 0.34625),\n",
              " (1.0984701706666111, 0.34625),\n",
              " (1.098468611870834, 0.34625),\n",
              " (1.098467063488883, 0.34625),\n",
              " (1.098465525451899, 0.34625),\n",
              " (1.0984639976948478, 0.34625),\n",
              " (1.0984624801644989, 0.34625),\n",
              " (1.0984609727826389, 0.34625),\n",
              " (1.0984594754804915, 0.34625),\n",
              " (1.098457988214807, 0.34625),\n",
              " (1.0984565109093523, 0.34625),\n",
              " (1.0984550434778024, 0.34625),\n",
              " (1.0984535858544247, 0.34625),\n",
              " (1.0984521379742558, 0.34625),\n",
              " (1.0984506997785102, 0.34625),\n",
              " (1.0984492712241165, 0.34625),\n",
              " (1.0984478522402954, 0.34625),\n",
              " (1.0984464427435499, 0.34625),\n",
              " (1.0984450426706989, 0.34625),\n",
              " (1.0984436519592309, 0.34625),\n",
              " (1.09844227055445, 0.34625),\n",
              " (1.098440898413337, 0.34625),\n",
              " (1.0984395354624794, 0.34625),\n",
              " (1.0984381816332116, 0.34625),\n",
              " (1.0984368368751982, 0.34625),\n",
              " (1.0984355011119913, 0.34625),\n",
              " (1.0984341742940995, 0.34625),\n",
              " (1.0984328563616272, 0.34625),\n",
              " (1.0984315472455979, 0.34625),\n",
              " (1.09843024688762, 0.34625)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}